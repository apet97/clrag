# Company AI Setup - Local AI Model Integration

**Clockify RAG System configured for internal company AI model**

This guide explains how to use the company AI model at `10.127.0.192:11434` (gpt-oss:20b) with the RAG system.

---

## üöÄ Quick Start (Company AI)

### Prerequisites
- ‚úÖ VPN connection (required to access internal endpoint)
- ‚úÖ RAG system installed locally
- ‚úÖ Company AI endpoint running (verify via `curl http://10.127.0.192:11434/api/tags`)

### One-Liner Setup
```bash
# Copy company AI configuration
MODEL_BASE_URL="http://10.127.0.192:11434" \
MODEL_NAME="gpt-oss:20b" \
make crawl preprocess chunk embed hybrid
```

### Or Standard Setup
Edit `.env` and set:
```bash
MODEL_BASE_URL=http://10.127.0.192:11434
MODEL_NAME=gpt-oss:20b
MODEL_API_KEY=  # Leave empty - no API key needed
```

Then run:
```bash
source .venv/bin/activate
make crawl preprocess chunk embed hybrid
make serve
```

---

## üîå Available Models

Company AI hosts multiple models. Here are available options (as of latest sync):

### Recommended for RAG
| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| **gpt-oss:20b** | 13.7 GB | Medium | High | **RECOMMENDED** - Best balance |
| qwen2.5:32b | 19.8 GB | Slow | Very High | Accuracy critical tasks |
| deepseek-r1:70b | 42.5 GB | Very Slow | Excellent | Complex reasoning |
| gemma3:27b | 17.4 GB | Medium | High | Alternative to gpt-oss |

### Lightweight Options
| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| llama3.2:3b | 2.0 GB | Fast | Good | Quick tests, low resources |
| qwen2.5-coder:1.5b | 986 MB | Very Fast | Good | Code-focused queries |

### Specialized Models
| Model | Size | Purpose |
|-------|------|---------|
| llava:13b | 8.0 GB | Vision + text (image understanding) |
| mvkvl/sentiments:mistral | 4.1 GB | Sentiment analysis |
| nomic-embed-text:latest | 274 MB | Embeddings (used by default) |

---

## ‚úÖ Verify Company AI Connection

### Test 1: Get Available Models
```bash
curl http://10.127.0.192:11434/api/tags | jq
```

Expected output: JSON list of available models

### Test 2: Send Test Message
```bash
curl -X POST http://10.127.0.192:11434/api/chat \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "gpt-oss:20b",
    "messages": [{"role": "user", "content": "ping"}],
    "stream": false
  }' | jq .message.content
```

Expected output: `"pong"` or similar response

### Test 3: Check RAG System Health
```bash
# After starting the RAG system
curl http://localhost:7000/health | jq

# Should show: "status": "healthy"
```

---

## üõ†Ô∏è IDE Integration (Direct Connection)

If you want to use the company AI model directly in your IDE (without the RAG system):

### Connection Details
- **Endpoint**: `10.127.0.192:11434`
- **API Format**: Ollama-compatible
- **Authentication**: None (no API key needed)
- **Port**: 11434

### IDE Integration Examples

**VS Code with Continue.dev**
```json
{
  "models": [
    {
      "title": "Company GPT-OSS 20B",
      "provider": "ollama",
      "model": "gpt-oss:20b",
      "apiBase": "http://10.127.0.192:11434"
    }
  ]
}
```

**JetBrains IDEs with Ollama Plugin**
1. Install Ollama plugin
2. Settings ‚Üí Ollama
3. Server URL: `http://10.127.0.192:11434`
4. Model: `gpt-oss:20b`

**Terminal Usage**
```bash
# Set as default
export OLLAMA_HOST=http://10.127.0.192:11434

# Run chat
ollama run gpt-oss:20b "Your question here"
```

---

## üìä Performance Expectations

**‚ö†Ô∏è Important Notes:**
- This is a **Proof-of-Concept (PoC)** deployment
- Expect **moderate performance** and **occasional slowdowns**
- First request may take 20-30 seconds (model loading)
- Subsequent requests: 5-15 seconds depending on complexity

### Typical Latencies
| Operation | Time | Notes |
|-----------|------|-------|
| First request | 20-30s | Model initialization |
| Chat response | 5-15s | LLM generation |
| Search | 50-100ms | FAISS vector search |
| RAG pipeline | 10-25s | Search + LLM combined |

### Memory & Resource Usage
- Model loaded in VRAM: ~20-24GB (gpt-oss:20b)
- Shared system resource - performance may vary with other users
- Rate limiting: Not yet implemented (be respectful of shared resource)

---

## üéØ Use Cases & Recommendations

### Good For
‚úÖ Testing RAG functionality
‚úÖ Development and experimentation
‚úÖ Non-time-critical tasks
‚úÖ Trying different prompts/queries
‚úÖ Integration testing

### Not Recommended For
‚ùå Production high-traffic systems (use external API instead)
‚ùå Real-time chat applications
‚ùå Batch processing many requests
‚ùå Latency-sensitive applications

---

## üîÑ Model Selection

### Switch to Different Model
Edit `.env`:
```bash
MODEL_NAME=qwen2.5:32b  # Or any other available model
```

Restart the RAG system:
```bash
make serve
```

### Compare Model Performance
```bash
#!/bin/bash
for model in gpt-oss:20b qwen2.5:32b gemma3:27b; do
  echo "Testing: $model"
  time curl -X POST http://10.127.0.192:11434/api/chat \
    -H 'Content-Type: application/json' \
    -d "{\"model\":\"$model\",\"messages\":[{\"role\":\"user\",\"content\":\"What is RAG?\"}],\"stream\":false}" \
    | jq -r .message.content
  echo "---"
done
```

---

## üìà Feedback & Monitoring

### Collecting Feedback for PoC Evaluation

Please provide feedback on the company AI using this template:

**Date**: ___________
**Model Used**: ___________
**Task**: ___________ (e.g., search, chat, coding)

#### Feedback Questions (Rate 1-5, 1=Poor, 5=Excellent)

**Performance**
- Speed: ‚òê1 ‚òê2 ‚òê3 ‚òê4 ‚òê5
- Reliability: ‚òê1 ‚òê2 ‚òê3 ‚òê4 ‚òê5
- Stability: ‚òê1 ‚òê2 ‚òê3 ‚òê4 ‚òê5

**Quality**
- Answer Accuracy: ‚òê1 ‚òê2 ‚òê3 ‚òê4 ‚òê5
- Answer Usefulness: ‚òê1 ‚òê2 ‚òê3 ‚òê4 ‚òê5
- Relevance: ‚òê1 ‚òê2 ‚òê3 ‚òê4 ‚òê5

**Overall Experience**
- Would Use Again: ‚òê1 ‚òê2 ‚òê3 ‚òê4 ‚òê5
- Recommendation: ‚òê1 ‚òê2 ‚òê3 ‚òê4 ‚òê5
- Overall Satisfaction: ‚òê1 ‚òê2 ‚òê3 ‚òê4 ‚òê5

**Comments/Issues**:
```
[Your feedback here]
```

**Send feedback to**: [Internal feedback channel TBD]

### Monitoring Usage

Track your usage to help identify patterns:

```bash
#!/bin/bash
# Monitor queries to company AI
echo "Company AI Usage Summary"
echo "======================================="
curl -s http://10.127.0.192:11434/api/tags | jq '.models | length'
echo "Total models available"

# Log request to a file for feedback
echo "[$(date)] Query to gpt-oss:20b via RAG" >> ~/.config/company-ai-usage.log
```

---

## üîê Security & Best Practices

### What's Safe to Test
‚úÖ Internal company data
‚úÖ Clockify help articles
‚úÖ Development code
‚úÖ Internal documentation
‚úÖ Test data

### What's NOT Safe to Test
‚ùå Production credentials
‚ùå Customer personal data
‚ùå Payment information
‚ùå Sensitive business secrets
‚ùå External API keys

**Note**: The PoC is specifically designed for testing - be mindful but feel free to experiment!

---

## üÜò Troubleshooting

### Connection Refused
```bash
# Check if endpoint is accessible
curl -v http://10.127.0.192:11434/api/tags

# Verify VPN connection
ping 10.127.0.192
```

### Model Not Available
```bash
# List all available models
curl http://10.127.0.192:11434/api/tags | jq '.models[].name'

# Pull a model if missing
# (Note: May not work on shared instance - ask admin)
ollama pull gpt-oss:20b
```

### Slow Response Times
```bash
# Check if model is loaded
curl http://10.127.0.192:11434/api/tags | jq '.models[] | select(.name=="gpt-oss:20b")'

# Test with smaller model
MODEL_NAME=llama3.2:3b make serve
```

### "Out of Memory" Errors
- The shared instance has memory constraints
- Try smaller model: `llama3.2:3b` or `qwen2.5-coder:1.5b`
- Contact admin if persistent

---

## üìö Additional Resources

- **RAG Setup**: See [STEP_BY_STEP_GUIDE.md](STEP_BY_STEP_GUIDE.md)
- **API Reference**: See [API_DOCUMENTATION.md](API_DOCUMENTATION.md)
- **Local Ollama Setup**: See [STEP_BY_STEP_GUIDE.md - Local LLM Setup](STEP_BY_STEP_GUIDE.md#local-llm-setup)
- **Company AI Web UI**: https://ai.coingdevelopment.com (VPN required, "Continue with Cake ID")

---

## üìù Configuration Reference

### Complete `.env` for Company AI
```bash
# ==== COMPANY AI CONFIGURATION ====
MODEL_BASE_URL=http://10.127.0.192:11434
MODEL_API_KEY=
MODEL_NAME=gpt-oss:20b
MODEL_MAX_TOKENS=1000
MODEL_TEMPERATURE=0.7

# ==== EMBEDDING CONFIGURATION ====
# Can use company embedding or external
EMBEDDING_MODEL=nomic-embed-text:latest
EMBEDDING_BATCH_SIZE=32
EMBEDDING_POOL_SIZE=4

# ==== CRAWLING (Optional - customize for your use) ====
CRAWL_BASES=https://clockify.me/help,https://python.langchain.com/docs
DOMAINS_WHITELIST=clockify.me,langchain.com
CRAWL_ALLOW_OVERRIDE=false

# ==== PIPELINE FEATURES ====
PARENT_CHILD_INDEXING=true
HYBRID_SEARCH=true
QUERY_REWRITES=true
USE_RERANKER=true

# ==== LOGGING ====
DEBUG=false
```

---

## ‚úÖ Checklist: Company AI Setup

- ‚òê VPN connection verified
- ‚òê `.env` updated with company AI endpoint
- ‚òê Connection test passed: `curl http://10.127.0.192:11434/api/tags`
- ‚òê RAG system started: `make serve`
- ‚òê Health check passed: `curl http://localhost:7000/health`
- ‚òê First search test successful
- ‚òê Ready to provide feedback

---

## üéâ You're Ready!

Your RAG system is now configured for the company AI model.

**Next Steps:**
1. Run the RAG pipeline: `make crawl preprocess chunk embed hybrid`
2. Start the server: `make serve`
3. Test searches and chat
4. Provide feedback on performance and usefulness

**Questions?** Contact [admin/support] or check [internal documentation]

---

**Last Updated**: October 20, 2025
**Status**: PoC Active
**Endpoint**: http://10.127.0.192:11434
**Recommended Model**: gpt-oss:20b
**Web UI**: https://ai.coingdevelopment.com (VPN required)
