# Clockify RAG System Configuration

# ============================================================================
# PERSONAL PC SETUP (for development/testing with mock LLM)
# ============================================================================

# Use mock LLM mode (instant responses, no real LLM needed)
MOCK_LLM=true
LLM_ENDPOINT=http://localhost:8080/v1
LLM_MODEL=oss20b
LLM_API_TYPE=openai
FAISS_ENDPOINT=http://localhost:8888

# ============================================================================
# WORK LAPTOP SETUP (for production with company Ollama)
# ============================================================================

# ⚠️  CRITICAL: Get these values from your IT team or DevOps

# Company Ollama endpoint (via HTTPS at company IP)
# Ask your team: "What's the Ollama endpoint URL?"
# Format is usually: https://[ip-address]:port/api/chat
LLM_ENDPOINT=https://your-company-ip-address/api/chat

# Model name (confirm with your team)
LLM_MODEL=gpt-oss20b

# API type: "ollama" for company setup, "openai" for local Ollama
LLM_API_TYPE=ollama

# Disable mock mode to use real LLM
MOCK_LLM=false

# FAISS endpoint (local retrieval)
FAISS_ENDPOINT=http://localhost:8888

# API server configuration
API_PORT=8000
API_HOST=0.0.0.0

# Logging
LOG_LEVEL=INFO

# ============================================================================
# OPTIONAL: Advanced Configuration
# ============================================================================

# LLM Request Timeout (seconds)
LLM_TIMEOUT=60

# LLM Retry Attempts
LLM_MAX_RETRIES=3

# Batch size for embedding
EMBEDDING_BATCH_SIZE=32

# ============================================================================
# COMPANY-SPECIFIC NOTES
# ============================================================================

# 1. Get Ollama Endpoint URL
#    Ask: "What's the Ollama endpoint for gpt-oss20b?"
#    Typical format: https://192.168.1.100:8080/api/chat
#    Update: LLM_ENDPOINT=<your-url>

# 2. Verify API Type
#    Company hosts Ollama → use api_type="ollama"
#    If OpenAI-compatible API → use api_type="openai"

# 3. SSL Certificate Issues
#    If you get SSL verification errors:
#    - Ask IT if certificate is self-signed
#    - Code already handles self-signed with verify=False
#    - If still issues, contact IT

# 4. Authentication
#    If Ollama requires API key:
#    - Let us know, we can add header support
#    - Typical header: Authorization: Bearer <token>

# 5. Test the Connection
#    Before deploying, test:
#    curl -X POST https://[your-ip]/api/chat \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "gpt-oss20b",
#        "messages": [{"role": "user", "content": "say hello"}],
#        "stream": false
#      }'

# ============================================================================
# TROUBLESHOOTING
# ============================================================================

# Problem: "Cannot connect to LLM"
# Solution:
#   1. Verify LLM_ENDPOINT is correct
#   2. Test with curl command above
#   3. Check if company requires VPN connection
#   4. Contact IT for endpoint verification

# Problem: "SSL: CERTIFICATE_VERIFY_FAILED"
# Solution:
#   - Code already disables SSL verification for company certs
#   - If error persists, contact IT

# Problem: "Unexpected response format"
# Solution:
#   - May be different Ollama version
#   - Let us know response format
#   - We can update request/response parsing

# Problem: "Rate limit exceeded"
# Solution:
#   - Company may have usage limits
#   - Ask IT about rate limits/quotas
#   - May need to adjust LLM_MAX_RETRIES or timeouts
