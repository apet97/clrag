# Clockify RAG Configuration
# Copy this to .env and fill in your values

# ============================================================================
# LLM CONFIGURATION (Ollama or OpenAI-compatible)
# ============================================================================

# API Type: "ollama" or "openai"
# - ollama: Internal Ollama instance at http://<INTERNAL_OLLAMA_HOST>:11434
# - openai: OpenAI-compatible API (with Bearer token)
LLM_API_TYPE=ollama

# Model name (with colon for proper Ollama formatting)
# Ollama: gpt-oss:20b
# OpenAI: gpt-4, gpt-3.5-turbo, etc.
LLM_MODEL=gpt-oss:20b

# OpenAI API Key (only if LLM_API_TYPE=openai)
# Leave empty for Ollama (no key needed)
OPENAI_API_KEY=

# Verify SSL certificates (set to false for self-signed or internal certs)
LLM_VERIFY_SSL=false

# Mock LLM mode (for development without real LLM)
# true: Use mock responses (instant, no real API calls)
# false: Use real LLM endpoint
MOCK_LLM=false

# Streaming support (Ollama SSE aggregation). Off by default.
STREAMING_ENABLED=false

# Environment: dev, staging, prod
ENV=dev

# Optional admin token to reveal sensitive config fields in /config when ENV=prod
ADMIN_TOKEN=change-me

# Deprecated alias (back-compat): if set and LLM_TIMEOUT_SECONDS is not, uses this with warning.
# LLM_TIMEOUT=30

# LLM endpoint base URL (separate from chat path for flexibility)
# Ollama (internal): http://<INTERNAL_OLLAMA_HOST>:11434
# Ollama (local dev): http://localhost:11434
# Reverse proxy HTTPS: https://ai.company.tld
LLM_BASE_URL=http://localhost:11434

# LLM API paths (configurable for reverse proxies)
# Ollama default: /api/chat and /api/tags
# OpenAI default: /v1/chat/completions (chat_path) and /v1/models (tags_path)
LLM_CHAT_PATH=/api/chat
LLM_TAGS_PATH=/api/tags

# LLM request timeout in seconds
LLM_TIMEOUT_SECONDS=30

# LLM API authentication (Bearer token for OpenAI-compatible APIs)
# Example: sk-... (OpenAI), or internal bearer token for authenticated reverse proxy
LLM_BEARER_TOKEN=

# LLM retry configuration (exponential backoff with jitter)
# Number of retries for transient errors (timeouts, 5xx)
LLM_RETRIES=3
# Initial backoff delay in seconds (doubled exponentially per retry)
LLM_BACKOFF=0.75

# ============================================================================
# INDEX CONFIGURATION
# ============================================================================

# Index mode: "single" or "separate"
# - single: One FAISS index with namespace field in metadata
# - separate: Separate indexes per namespace
INDEX_MODE=single

# Namespaces to support (comma-separated)
NAMESPACES=clockify,langchain

# ============================================================================
# RETRIEVAL CONFIGURATION
# ============================================================================

# Number of results to retrieve
RETRIEVAL_K=5

# Enable cross-encoder reranking
RERANKER_ENABLED=false

# Model for reranking (if RERANKER_ENABLED=true)
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Expand chunks with parent context
PARENT_EXPANSION=false

# ============================================================================
# GLOSSARY & HYBRID RETRIEVAL CONFIGURATION
# ============================================================================

# Path to glossary CSV file (term,aliases,type,notes)
GLOSSARY_PATH=data/glossary.csv

# Hybrid retrieval fusion weight (0=BM25 only, 1=dense only, 0.6=balanced)
# Formula: final_score = HYBRID_ALPHA * dense_score + (1 - HYBRID_ALPHA) * bm25_score
HYBRID_ALPHA=0.6

# Number of results to fetch from dense retrieval
K_DENSE=40

# Number of results to fetch from BM25 lexical search
K_BM25=40

# Number of final results to return after fusion
K_FINAL=12

# ============================================================================
# CHUNKING CONFIGURATION
# ============================================================================

# Target chunk size in tokens (for child chunks)
CHUNK_TARGET_TOKENS=800

# Chunk overlap as percentage (0.0 to 1.0, defaults to 0.15 = 15%)
CHUNK_OVERLAP_PERCENTAGE=0.15

# Max chunk size in tokens (for procedural pages with many steps)
CHUNK_MAX_TOKENS=1200

# Overlap between chunks in tokens (legacy, used if CHUNK_OVERLAP_PERCENTAGE not set)
CHUNK_OVERLAP_TOKENS=120

# Parent chunk tokens (for sectional grouping)
PARENT_CHUNK_TOKENS=3500

# Parent chunk overlap tokens
PARENT_CHUNK_OVERLAP_TOKENS=300

# ============================================================================
# API SERVER CONFIGURATION
# ============================================================================

# Server host and port
API_HOST=0.0.0.0
API_PORT=7000

# ============================================================================
# LOGGING
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ============================================================================
# DEVELOPMENT/DEBUGGING
# ============================================================================

# Enable debug logging for LLM requests
DEBUG_LLM=false

# Seed for reproducible results (optional)
SEED=42
