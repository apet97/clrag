# Clockify RAG Configuration
# Copy this to .env and fill in your values

# ============================================================================
# LLM CONFIGURATION (Ollama or OpenAI-compatible)
# ============================================================================

# API Type: "ollama" or "openai"
# - ollama: Company AI at http://10.127.0.192:11434/api/chat
# - openai: OpenAI-compatible API (with Bearer token)
LLM_API_TYPE=ollama

# LLM Endpoint
# Ollama (company): http://10.127.0.192:11434/api/chat
# OpenAI: https://api.openai.com/v1
LLM_ENDPOINT=http://10.127.0.192:11434/api/chat

# Model name (with colon for proper Ollama formatting)
# Ollama: gpt-oss:20b
# OpenAI: gpt-4, gpt-3.5-turbo, etc.
LLM_MODEL=gpt-oss:20b

# OpenAI API Key (only if LLM_API_TYPE=openai)
# Leave empty for Ollama (no key needed)
OPENAI_API_KEY=

# Verify SSL certificates (set to false for self-signed or internal certs)
LLM_VERIFY_SSL=false

# Mock LLM mode (for development without real LLM)
# true: Use mock responses (instant, no real API calls)
# false: Use real LLM endpoint
MOCK_LLM=false

# Streaming support (Ollama SSE aggregation). Off by default.
STREAMING_ENABLED=false

# Environment: dev, staging, prod
ENV=dev

# Optional admin token to reveal sensitive config fields in /config when ENV=prod
ADMIN_TOKEN=change-me

# Deprecated alias (back-compat): if set and LLM_TIMEOUT_SECONDS is not, uses this with warning.
# LLM_TIMEOUT=30

# LLM endpoint base URL (separate from chat path for flexibility)
# Ollama (company VPN): http://10.127.0.192:11434
# Ollama (local): http://localhost:11434
# Reverse proxy HTTPS: https://ai.company.tld
LLM_BASE_URL=http://10.127.0.192:11434

# LLM API paths (configurable for reverse proxies)
# Ollama default: /api/chat and /api/tags
# OpenAI default: /v1/chat/completions (chat_path) and /v1/models (tags_path)
LLM_CHAT_PATH=/api/chat
LLM_TAGS_PATH=/api/tags

# LLM request timeout in seconds
LLM_TIMEOUT_SECONDS=30

# ============================================================================
# INDEX CONFIGURATION
# ============================================================================

# Index mode: "single" or "separate"
# - single: One FAISS index with namespace field in metadata
# - separate: Separate indexes per namespace
INDEX_MODE=single

# Namespaces to support (comma-separated)
NAMESPACES=clockify,langchain

# ============================================================================
# RETRIEVAL CONFIGURATION
# ============================================================================

# Number of results to retrieve
RETRIEVAL_K=5

# Enable cross-encoder reranking
RERANKER_ENABLED=false

# Model for reranking (if RERANKER_ENABLED=true)
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Expand chunks with parent context
PARENT_EXPANSION=false

# ============================================================================
# CHUNKING CONFIGURATION
# ============================================================================

# Target chunk size in tokens
CHUNK_TARGET_TOKENS=1000

# Overlap between chunks in tokens
CHUNK_OVERLAP_TOKENS=150

# ============================================================================
# API SERVER CONFIGURATION
# ============================================================================

# Server host and port
API_HOST=0.0.0.0
API_PORT=7000

# ============================================================================
# LOGGING
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ============================================================================
# DEVELOPMENT/DEBUGGING
# ============================================================================

# Enable debug logging for LLM requests
DEBUG_LLM=false

# Seed for reproducible results (optional)
SEED=42
